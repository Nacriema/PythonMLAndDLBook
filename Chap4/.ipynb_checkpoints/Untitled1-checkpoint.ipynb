{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 3: Linear Methods for Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Introduction \n",
    "\n",
    "Một model hồi quy tuyến tính (linear regression model) giả định rằng hàm hồi quy E(Y|X) là tuyến tính đối với các input $X_{1}, ..., X_{p}$. Mô hình tuyến tính được phát triển từ giai đoạn sớm của thống kê, đến ngày nay nó vẫn quan trọng và cần thiết việc nghiên cứu về nó. Chúng đơn giản và thường giải thích được cách mà đầu vào ảnh hưởng đến đầu ra. Cho mục đích dự đoán, đôi khi nó vượt trội hơn so với những mô hình phi tuyến, đặc biệt là khi ta có số lượng trainint data nhỏ, low signal-to-noise ratio hoặc là dữ liệu thưa thớt. Cuối cùng, phương thức tuyến tính có thể áp dụng để biến đổi cho inputs và được gọi là mở rộng scope của chúng. Những phương pháp tổng quát này đôi khi được gọi là basic-function methods, và được thảo luận trong chapter 5.\n",
    "\n",
    "Trong chương này, ta mô tả phương thức tuyến tính cho hồi quy, và trong chương sau, ta sẽ nói phương thức tuyến tính cho phân loại. Chú ý là hiểu được bên linear methods này là cần thiết để hiểu những mô hình nonlinear ở phía sau. Thực tế là nhiều kỹ thuật phi tuyến được tổng quan từ các phương thức tuyến tính được thảo luận ở đây.\n",
    "\n",
    "## 3.2 Linear Regression Models and Least Squares\n",
    "\n",
    "Như đã giới thiệu trong chương 2, ta có vector input X = ($X_{1}, X_{2}, ..., X_{p}$), và ta muốn dự đoán giá trị thực của Y. Mô hình hồi quy tuyến tính có dạng:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://latex.codecogs.com/gif.latex?f%28X%29%3D%5Cbeta_%7B0%7D%20&plus;%20%5Csum_%7Bj%3D1%7D%5E%7Bp%7DX_%7Bj%7D%5Cbeta_%7Bj%7D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mô hình tuyến tính giả định rằng hàm hồi quy E(Y|X) là tuyến tính, hoặc là mô hình này là xấp xỉ tuyến tính. Ở đây, các giá trị $\\beta_{j}$ kà các tham số chưa biết (unknown parameters) hay còn gọi là các hệ số (coefficient), và các biến $X_{j}$ có thể đến từ các nguồn khác nhau:\n",
    "* Đầu vào tịnh lượng được (quantiative input)\n",
    "* Dạng biến đổi của quantiative input, như là log, square-root hay là bình phương\n",
    "* Mở rộng cơ bản, ví dụ $X_2 = X_{1}^{2}, X_{3} = X_{1}^{3}$, dẫn đến dạng biểu diễn đa thức\n",
    "* \"dummy\" bậc của quantitative inputs. Ví dụ, nếu G là five-level factor input, ta có thể tạo $X_{j}$, j = 1..5 sao cho $X_{j} = I (G = j)$. Tập các X_{j} biểu diễn ảnh hưởng của G bằng tập hợp các hằng phụ thuộc (level-dependent constants), do đó trong biểu thức $\\sum_{j=1}^{5}X_{j}\\beta_{j}$, một giá trị của $X_{j}$ là 1, và các giá trị khác là bằng 0 (ta có thể thấy nó giống như dạng one hot encoding với dữ liệu nomial variable)\n",
    "* Tương tác giữa các biến, ví dụ: $X_{3} = X_{1}.X_{2}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Với bất kỳ nguồn nào của $X_{j}$, model vẫn là tuyến tính với các tham số đó.\n",
    "\n",
    "Tương tự ta có tập giá trị của dữ liệu train $(x_{1}, y_{1}), ...,(x_{n}, y_{n})$, từ chúng ta ước lượng các giá trị của tham số $\\beta$. Mỗi $x_{i} = (x_{i1}, x_{i2}, ..., x_{ip})^{T}$ là một vector của thuộc tính đo đạc cho trường hợp thứ i. Phương pháp ước lượng phổ biến nhất là **least squares** (bình phương nhỏ nhất), trong đó ta chọn hệ số $\\beta = (\\beta_{0}, \\beta_{1},..., \\beta_{p})^{T}$ để giảm thiểu tổng các hiệu bình phương sau (**R**esidual\n",
    "**S**um of **S**quares):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://latex.codecogs.com/gif.latex?RSS%28%5Cbeta%29%20%3D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%28y_%7Bi%7D-f%28x_%7Bi%7D%29%29%5E%7B2%7D%20%3D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%28y_%7Bi%7D%20-%20%5Cbeta_%7B0%7D%20-%20%5Csum_%7Bj%3D1%7D%5E%7Bp%7Dx_%7Bij%7D%5Cbeta_%7Bj%7D%29%5E%7B2%7D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở góc nhìn thống kê, tiêu chí này là có lý bởi vì nếu dữ liệu training ($x_{i}, y_{i}$) đại diện cho một thằng được chọn ngẫu nhiên từ quần thể của nó. Ngay cả khi các $x_{i}$ không được chọn ngẫu nhiên, tiêu chí đánh giá vẫn hợp lệ nếu như các $y_{i}$ là độc lập có điều kiện với đầu vào $x_{i}$. Hình bên dưới mô tả hình học của least-square fitting trong tọa độ $\\mathbb{R}^{p+1}$ chiều."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](RSS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Không gian được chiếm bởi các cặp (X, Y). Chú ý rằng biểu thức (3.2) không có giả thuyết nào về tính hợp cho của hàm (3.1); nó đơn giản là tìm một siêu phẳng tuyến tính mà fit được dữ liệu. Least square fitting là thỏa mãn về trực giác cho dù data có như thế nào đi nữa; tiêu chí đánh giá tính trung bình thiếu độ fit.\n",
    "\n",
    "Làm thế nào để ta rút gọn biểu thức (3.2). Ký hiệu X là ma trận N * (p + 1) với mỗi hàng là một input vector (với 1 là giá trị tại chỉ số 0 của nó), và tương tự ký hiệu y là vector N chiều (N-vector output) đầu ra. Ta có thể viết lại công thức cho Residual sum of squares như sau: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://latex.codecogs.com/gif.latex?RSS%28%5Cbeta%29%20%3D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%28y_%7Bi%7D%20-%20X_%7Bi%7D%5Cbeta%29%5E%7B2%7D%3D%28y%20-%20X%5Cbeta%29%5E%7BT%7D%28y%20-%20X%5Cbeta%29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đây là phương trình bậc 2 với p + 1 tham số, đạo hàm riêng phần theo $\\beta$ ta được: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://latex.codecogs.com/gif.latex?%5Cfrac%7B%5Cpartial%20RSS%7D%7B%5Cpartial%20%5Cbeta%7D%20%3D%20-2X%5E%7BT%7D%28y%20-%20X%5Cbeta%29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chứng minh điều này:\n",
    "\n",
    "$RSS(\\beta) = y^{T}y - y^{T}X\\beta - \\beta^{T}X^{T}y + \\beta^{T}X^{T}X\\beta$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$= y^{T}y - 2y^{T}X\\beta + \\beta^{T}X^{T}X\\beta$  ( chú ý là: $A^{T}B = BA^{T}$ (Sử dụng đến tính chất: $(AB)^{T} = B^{T}A^{T}$)) \n",
    "\n",
    "Sử dụng matrix diffrentiation: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial }{\\partial \\beta}{y^{T}X\\beta} = y^{T}X$\n",
    "\n",
    "$\\frac{\\partial }{\\partial \\beta}{\\beta ^ {T}X ^ {T}X\\beta} = 2X^{T}X\\beta$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giả sử là X full column rank (tức là các cột trong X là độc lập tuyến tính - Không phụ thuộc lẫn nhau) và do đó $X^{T}X$ là positive definite, ta muốn cực tiểu hóa $RSS(\\beta)$, giờ ta cho đạo hàm theo $\\beta$ = 0, tức là: \n",
    "\n",
    "$X^{T}(y - X\\beta) = 0$\n",
    "\n",
    "và từ đó suy ra được giá trị của xấp xỉ cho $\\beta$:\n",
    "\n",
    "$\\hat{\\beta} = (X^{T}X)^{-1}X^{T}y$\n",
    "\n",
    "Chứng minh, ví dụ có: \n",
    "AX = C\n",
    "$\\Rightarrow A^{-1}AX = A^{-1}C$\n",
    "$\\Rightarrow IX = A^{-1}C$\n",
    "$\\Rightarrow X = A^{-1}C$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pro.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giá trị dự đoán cho vector đầu vào $x_{0}$ được định nghĩa bằng: $\\hat{f}(x_{0}) = (1:x_{0})^{T}\\hat{\\beta}$, và do đó giá trị fit value dự đoán cho dữ liệu train đầu vào là: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{y} = X\\hat{\\beta} = X(X^{T}X)^{-1}X^{T}y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Với ký hiệu $\\hat{y}_{i} = \\hat{f}(x_{i})$. Ma trận $H =X(X^{T}X)^{-1}X^{T}$ xuất hiện trong biểu thức trên đôi khi được gọi là \"hat\" matrix bởi vì nó cho kết quả là $\\hat{y}$. Hình 3.2 là biểu diễn hình học của least square estimate. Ta ký hiệu các cột của vector X là các giá trị $x_{0}, x_{1}, ..., x_{p}$ với $x_{0} \\equiv 1$. Ta tối thiểu hóa biểu thức $RSS(\\beta)= \\left \\| y - X\\beta \\right \\|^{2}$ bằng cách chọn giá trị $\\hat{\\beta}$ sao cho vector tối ưu $y - \\hat{y}$ là trực giao với mặt phẳng subspace này (ví dụ ta chọn x1, x2). Ma trận H còn được gọi là projection matrix. Có thể xảy ra trường hợp các cột của X là không độc lập tuyến tính với nhau, do đó X không phải là full rank. Điều này có thể xảy ra, ví dụ, nếu như 2 cột của đầu vào là **perfectly correlated** (ví dụ: $x_{2} = 3x_{1}$). Thì matrận vuông $X^{T}X$ là singular (tức không khả nghịch) và hệ số least square $\\hat{\\beta}$ không phải là duy nhất. Tuy nhiên giá trị $\\hat{y} = X\\hat{\\beta}$ vẫn là phép chiếu của y lên trên không gian X; chỉ là có nhiều hơn 1 cách biểu diễn phép chiếu đó dưới dạng biểu diễn các cột của thằng X thôi. Trường hợp này xảy ra thường xuyên khi một hoặc nhiều đầu vào được mã hóa một cách dư thừa. Thường có một cách để giải quyết các đại lượng này là drop những cột dư thừa đi hoặc là mã hóa lại các cột trong X. Phần lớn các gói hồi quy tuyến tính đều phát hiên các dư thừa và tự động cài đặt một vài chiến lược để loại bỏ chúng. **Rank deficiencies** có thể xuất hiện trong tính hiệu và xử lý ảnh, khi mà số lượng các feature là vượt quá số lượng mẫu train N. Trong trường, những thuật tính này sẽ được bỏ đi hoặc làm gì gì đó (pahàn 5.2.3 và chap 16).\n",
    "\n",
    "Để xác định các thuộc tính lấy mẫu của $\\hat{\\beta}$, ta giả định rằng các mẫu quan sát $y_{i}$ là độc lập và có giá trị phương sai là hằng sô $\\sigma^{2}$, và các giá trị x_{i} là cố định (không random). Cái variance–covariance\n",
    "matrix of the least squares parameter estimates được suy ra là: \n",
    "\n",
    "$Var(\\hat{\\beta}) = (X^{T}X)^{-1}\\sigma^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Và một giá trị ước lượng của $\\sigma ^{2}$ là:\n",
    "\n",
    "$\\hat{\\sigma}^{2} = \\frac{1}{N-p-1}\\sum_{i=1}^{N}(y_{i} - \\hat{y_{i}})^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở đây ta sử dụng giá trị N - p - 1 thay vì N làm cho cái $\\hat{\\sigma}^{2}$ trở nên unbiased estimate của thăng chính xác $\\sigma ^ {2}$: E($\\hat{\\sigma ^ {2}}$) = $\\sigma^{2}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3",
   "language": "python",
   "name": "env3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
