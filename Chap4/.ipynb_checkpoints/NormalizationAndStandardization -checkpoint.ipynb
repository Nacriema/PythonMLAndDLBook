{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bringing features onto the same scale\n",
    "\n",
    "**Feature scaling** là bước quan trọng và cần thiế trong tiến trình xử lý mà ta hay bị lãng quên nó. **Decision tree** và **Random forests** là hai trong số ít những thuật toán machine learning mà ta không cần bận tâm đến feature scaling. Những thuật toán này được gọi là **scale invariant**. Tuy nhiên, phần lớn thuật toán machine learning và thuật toán tối ưu sẽ thực hiện tốt hơn nếu nhữ các feature có cùng tỉ lệ với nhau, giống như những gì ta đã thảo luận trong chương 2 của cuốn sách, khi ta cài đặt **gradient descent optimization**.\n",
    "\n",
    "Điều quan trọng của feature scaling có thể được mô tả bởi một ví dụ đơn giản. Giả sử rằng ta có 2 thuộc tính mà mỗi thuộc tính được đo với tí lệ từ 1 đến 10 và thuộc tính thứ 2 được đo đạc với tỉ lệ 1 đến 100000. Nếu ta nghĩ đến hàm **Squared Error** trong **ADALINE** từ chương 2, ta thấy rằng thuật toán sẽ khá bận rộn trong việc tối ưu trọng số bởi vì lượng error sẽ xuất hiện lớn hơn trong feature thứ 2. Một ví dụ khác là **K-nearest neighbors (KNN)** khi sử dụng độ đo **Eucliean**: khoảng cách đo đạc giữa các mẫu sẽ bị phụ thuộc hoàn toàn vào thuộc tính thứ hai.\n",
    "\n",
    "Bây giờ, ta có hai cách tiếp cận thông thường để mang các thuộc tính của chúng ta về lại scale chuẩn như nhau, đó là: **normalization** và **standardization**. Những thuật ngữ này được sử dụng khá lỏng lẻo trong các lĩnh vực khác nhau, và nghĩa của nó có thể thay đổi cho nhau trong từng trường hợp. Thông thường, **normalization** là phương thức chuyển hóa, rescaling các features về lại khoảng [0, 1], đó là trường hợp đặc biệt mang tên **min-max scaling** (min max normalization). Để normalize dữ liệu, đơn giản là áp dụng min-max scaling cho mỗi cột feature, nơi giá trị mới $x_{norm}^{(i)}$ của mẫu $x^{(i)}$ được tính toán theo công thức: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://latex.codecogs.com/gif.latex?x_%7Bnorm%7D%5E%7B%28i%29%7D%20%3D%20%5Cfrac%7Bx%5E%7B%28i%29%7D%20-%20x_%7Bmin%7D%7D%7Bx_%7Bmax%7D%20-%20x_%7Bmin%7D%7D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Với $x^{(i)}$ là mẫu thứ i, $x_{min}$ là gía trị nhỏ nhất trong mỗi feature column, và $x_{max}$ là giá trị lớn nhất.\n",
    "\n",
    "Quá trình min-max scaling được cài đặt bằng Sklearn và được sử dụng như sau: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   s1  s2\n",
       "0   1  10\n",
       "1   2   9\n",
       "2   3   8\n",
       "3   4   7\n",
       "4   5   6\n",
       "5   6   5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tao du lieu mẫu\n",
    "import pandas as pd\n",
    "s1 = pd.Series([1, 2, 3, 4, 5, 6], index=(range(6)))\n",
    "s2 = pd.Series([10, 9, 8, 7, 6, 5], index=(range(6)))\n",
    "df = pd.DataFrame(s1, columns=['s1'])\n",
    "df['s2'] = s2\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 1. ],\n",
       "       [0.2, 0.8],\n",
       "       [0.4, 0.6],\n",
       "       [0.6, 0.4],\n",
       "       [0.8, 0.2],\n",
       "       [1. , 0. ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Su dung Sklearn cho min-max normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "data_transform = mms.fit_transform(df.values)\n",
    "data_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mặc dù việc normalization thông tua min-max scaling là kỹ thuật sử dụng thông dụng và hữu ích khi ta muốn các giá trị nằm trong khoảng nhất định, nhưng **standardization** có thể vẫn là lựa chọn đầu tiền trong các thuật toán learning algorithm, đặc biệt là đối với các thuật toán tối ưu như là **gradient descent**. Lý do mà nhiều model tuyến tính (linear models) như là logistic regression và SVM trong chương 3, ta khởi tạo giá trị của các trọng số về 0 hoặc ccs số random ngẫu nhiên có giá trị gần với 0. Sử dụng standardization, ta căng chỉnh các feature columns về mean tại 0 và standard deviation là 1 và chuyển các cột thuộc tính về dạng phân phối chuẩn (zero mean và unit variance), điều này làm cho thuật toán học dễ dàng hơn. Hơn nữa, standardization duy trì được những thông tin hữu ích về những thằng \"ngoại lai\" (**outliers**) và làm cho thuật toán có thể nhận biết được những điểm như vậy. Ngược lại đối với thằng min-max scaling, thì range của dữ liệu scale được nằm trong khoảng giá trị nhất định mà thôi.\n",
    "\n",
    "Quá trình standardization được trình bày thông qua biểu thức: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://latex.codecogs.com/gif.latex?x_%7Bstd%7D%5E%7B%28i%29%7D%20%3D%20%5Cfrac%7Bx%5E%7B%28i%29%7D%20-%20%5Cmu_%7Bx%7D%7D%7B%5Csigma%20_%7Bx%7D%7D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở đây $\\mu_{x}$ là giá trị mean của một cột cụ thể, và $\\sigma_{x}$ tương ứng là giá trị standard deviation của cột đó. Nhắc lại công thức: $\\sigma_{x} = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(x_{i}-\\mu_{x})^{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bảng dưới đây mô tả điểm khác biệt giữa hai kỹ thuật scaling feature phổ biến, **standardization** và **normalization**, trên bảng dataset chứa tập number từ 1 đến 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3",
   "language": "python",
   "name": "env3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
